{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<h1>Advanced Data Science Capstone Project: <br>\n",
    "Traffic Forecast for a German News Website using SARIMA and LSTM</h1>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Table of contents\n",
    "1. [Introduction](#introduction)\n",
    "2. [Business Problem](#business_problem)\n",
    "3. [Data Aquisition, Selection & Wrangling](#data)\n",
    "4. [Methodology & Exploratory Data Analysis](#analysis)\n",
    "5. [Predictive Modeling using SARIMA and LSTM](#modelling)\n",
    "6. [Results,Conclusion/Discussion/Outlook](#results)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction <a name=\"introduction\"></a>\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "As part of the Advanced Data Science Capstone Project, I will try to forecast the Website Traffic of a News Website in Germany to gain insides on how much revenue, I can expect by showing ads to the customers on the Website within the next few months or even within the next year. This will be very helpful in understanding how much money I can invest in future projects or product features in accordance with the costs. There are different website metrics that determine the revenue of a website. For simplicity of the project, I will focus only on Visits*, because they are more robust to manipulation than PageViews or Clicks which can be influenced by click bait headlines or picture galleries. In this project, I'll focus on two different approaches to benchmark the performance of the traffic forecast. One will be the classical ML model SARIMA and the other will be a deep learning model called LSTM. \n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Business Problem <a name=\"business_problem\"></a>\n",
    "\n",
    "At the moment, the process of forecasting the Visits is done manually using a huge Excel file and is adjusted by a lot of stakeholders, who manually change the value due to their domain knowledge. This procedure is very time consuming, since it involves a lot of different stakeholders, who are responsible for the website's performance (eg. product management, editors, management, marketer, etc.), who then meet up in different rounds to discuss the numbers and finalize them. Once the numbers are finalized, the marketer gives an estimated revenue forecast. This inefficiency needs to be overcome by an automated process and a proper prediction model to save everyone's time and nerves. The reason for this is that so far, the Analytic's department provided the Excel file with a forecast based on a weighted average over the past 3 years using the change from the previous month and the month before that as a basic approach for the forecast model. Moreover, a constant residual is added to the model to smoothen the overall performance of the forecast, but for a News Website, who's traffic highly depends on the latest News, the website's traffic is very volatile over time and therefore the changes between the month are often too high for a suitable prediction of future values. To overcome this problem, I will try to use different forecast models to be more precise in the forecast.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Data Aquisition, Selection & Wrangling <a name=\"data\"></a>\n",
    "To begin with, I will collect the data that is necessary to tackle the problem. For simplicity reasons, I will start of by focusing only on the Visits per month for the website. This univariate times series approach will be discussed or adjusted at a later stage.\n",
    "\n",
    "#### Data Sources\n",
    "\n",
    "Due to the length of the time series, the numbers are not all in one place. Therefore, I will export the data from different sources and put them in one .csv file.\n",
    "For a continous implementation of the project, the data happens to be in Snowflake, where it was collected via the vendorâ€™s API. This happens on an hourly basis on a very detailed level provided as xml files for the various websites. The next step will be to gather the relevant information and aggregate the data to get daily numbers. This data is then mirrored to Foundry to work with it. \n",
    "\n",
    "\n",
    "<br>\n",
    "\n",
    "#### Data Selection\n",
    "\n",
    "Since the dataset includes more than just the one Website that I'm going to predict on, I need to make sure that I eliminate the data that is not relevant for the scope of this project. Because of the fact that I only have the time series data (date and visits for this single Website) available for such a long period, I don't have to decided whether a times series is correlated with another or explains parts of it or could be used as a feature to predict the chosen time series for the project. Therefore, I will work on an univariate Time Series.\n",
    "\n",
    "<br>\n",
    "\n",
    "#### Analysing the data \n",
    "To deep dive more into the analysis/data and to prepare the data for the algorithm, I will check whether the data has any missing values or not. This is important because it can have a huge impact on the model if the provided data has missing values due to it's time dependency. The missing information can have a linkage to the target label and an impact on the accuracy of the prediction. If there are missing values, I would interpolate the values using forward-filling, backward-filling or interpolation. Furthermore, I would check for outliers or values that are not in range of two standard deviations bandwidth or a similar approach to avoid having data available which is just not right (eg. having less data points during a day than expected due to tracking issues).\n",
    "\n",
    "## Methodology & Exploratory Data Analysis <a name=\"analysis\"></a>\n",
    "\n",
    "\n",
    "#### Preparing data for the models\n",
    "\n",
    "Once done with the data analysis, I would make sure that the data is in the right format for the chosen models. While the SARIMA model expects an input similar to a series, I will need to make sure that the date format is set properly. Moreover, I will check whether the times series is stationary or not by plotting the autocorrelation and using the adfuller test (ADF). The data is to be considered non-stationary if it's statistical properties like the mean and standard deviation are not constant over time but vary. The null hypothesis of the test is that it's non-stationary. That means in turn that if the p-value is less than the significance level, let's assume (0.05), then you would reject the null hypothesis and believe that the time series is stationary. Checking for seasonality by plotting the data is also important for chosing the right model and predict future values.\n",
    "\n",
    " \n",
    "\n",
    "## Predictive Modelling using SARIMA(X) and LSTM <a name=\"modelling\"></a>\n",
    "I will start with modelling and training a traditional SARIMA(X) model. SARIMA(X) stands for Seasonal Autoregressive Integrated Moving Average. With it's potential to handle seasonal data pretty well and combining an AR and MA model with differencing on top of it, it might be a good candiate for our times series predicition. And since it's widely used, it has the potential to be a model to make times series forecasts. Here it is crucial to find the right hyper parameter setting for the p,d,q-values as well as the p,d,q-values for the seasonal part of model. Once found the best hyper parameter combination, I will move on to investigate on the performance on a Deep Learning model before evaluating them. For that, I will train and model a simple LSTM (Long Short Term Memory).It's considered to be good model when it comes to Time Series Prediction. LSTM is a special form of an RNN. Due to it's capability to take past events into consideration and only store the information required over a period of time to use those to predict future values, it is highly appreciated to experiment with it to predict future values. For this, the right amount of neurons, epochs and the batch size are important hyperparameters that need to be tweaked to get good results. \n",
    "\n",
    " \n",
    "## Findings, Evaluation, Discussion & Outlook: <a name=\"results\"></a>\n",
    "After implementing a SARIMA and LSTM model and tweak it's parameter to find an optimum output, I will evaluate the model's result using RMSE, MSE, R2. Moreover, I will compare the predicted results and determine which model tends to be the best fit when predicting future values for the period given.\n",
    "Finally, I will give an outlook of what can be improved in the setup or which alternatives are available to further investigate on.\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
